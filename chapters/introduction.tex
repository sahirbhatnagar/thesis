%-----------------------------------------------------------------------------
\chapter{Introduction\label{ch:introduction}}
%-----------------------------------------------------------------------------

%Taken verbatim from Stephen Reid Tibs, Friedman~\cite{reid2016study} Consider the linear model $Y = X\beta + \varepsilon$,where Y is an n-vector of independently distributed responses, $X$ an $n \times p$ matrix with individual specific covariate vectors as its rows and $\varepsilon$ an $n$-vector of i.i.d. random variables (usually assumed Gaussian) each with mean 0 and variance $\sigma^2$ When $p > n$, one cannot estimate the unknown coefficient vector $\beta$ uniquely via standard least squares methodology. In fact, it is probably ill-advised to use least squares to estimate the vector even when $p \leq n$ with p close to n, since standard errors are likely to be high and parameter estimates unstable. In this instance, if one can assume that $\beta$ is reasonably sparse with many zero entries,

In this thesis, we consider the prediction of an outcome variable $y$ observed on $n$ individuals from $p$ variables, where $p$ is much larger than $n$. 
Challenges in this high-dimensional (HD) context include not only building a good predictor which will perform well in an independent dataset, but also being able to interpret the factors that contribute to the predictions. 
This latter issue can be very challenging in ultra-high dimensional predictor sets. 
For example, multiple different sets of covariates may provide equivalent measures of goodness of fit~\citep{fan2014challenges}, and therefore how does one decide which are important? 

When $p >>n$, standard generalized linear models (GLMs) methodology cannot uniquely estimate the unknown coefficient vector $\beta$. 
Moreover, even when $p \leq n$ with $p$ close to $n$, standard errors of GLMs are likely to be inflated and parameter estimates unstable~\citep{reid2016study}. In these instances it may be useful to assume the \textit{Bet on Sparsity Principle} which says:
\begin{quote}
Use a procedure that does well in sparse problems,
since no procedure does well in dense problems~\citep{friedman2001elements}.
\end{quote}
In fact, sometimes this assumption is necessary since we often do not have a large enough sample size to estimate so many parameters. Even when we do, we might want to identify a relatively small number of predictors that play an important role on the response. Applied researchers in the health sciences might prefer a simpler model because it can shed light on the etiology of disease. The sparsity assumption may also result in faster computations and more stable predictions on new datasets. 

With the advent of high-throughput technologies in genomics and brain imaging studies, computational approaches to variable selection have become increasingly important. Broadly speaking, there are three main approaches to analyze such HD data: 1) univariate regression followed by a filtering step often based on multiple testing corrections, and then subsequently a multivariable model using the reduced set of variables 2) multivariable penalized regression and 3) dimension reduction followed by a multivariable regression. 

In this thesis, we focus on the use of penalized regression methods for variable selection and prediction in HD settings. 
In Chapter 2, we provide a critical review of the literature on penalized regression methods and the computational algorithms used to fit these models. 
In Chapter 3, we develop the sparse additive interaction learning model \texttt{sail} for detecting non-linear interactions with a key environmental or exposure variable in HD data. 
In Chapter 4, we develop a general penalized linear mixed model framework called \texttt{ggmix} that simultaneously selects and estimates variables while accounting for between individual correlations in one step. 
Chapter 5 explores whether use of exposure-dependent clustering relationships in dimension reduction can improve predictive modelling in a two-step framework that we develop called \texttt{eclust}. 
Chapters 3, 4 and 5 were originally written as stand-alone papers and as a result, there is some inconsistency in notation and overlap with the literature review chapter. Chapter 5 has been published in \textit{Genetic Epidemiology}. 
Chapters 3 and 4 will be submitted for publication shortly after the submission of the thesis. 
We have published open source and freely available \texttt{R} packages for each of the methods developed Chapters 3, 4 and 5 (available at \url{https://github.com/greenwoodlab} and \url{https://github.com/sahirbhatnagar}). 
Table~\ref{tab:over} provides an overview of our software packages including some of their key characteristics. 
In Chapter 6, we conclude with an overview of the three manuscripts.  


\begin{comment}
\section{Overview of Our Software Packages}
	
\begin{itemize}
	\item \textbf{\footnotesize{}\texttt{eclust}}{\footnotesize{} \textendash{} Bhatnagar et al. (2017, Genetic Epidemiology)}\\
		{\footnotesize{}\url{https://cran.r-project.org/package=eclust}}{\footnotesize \par}
		\item \textbf{\footnotesize{}\texttt{sail}}{\footnotesize{} \textendash{} Bhatnagar, Yang and Greenwood
			(2018+, preprint)}\\
		{\footnotesize{}\url{https://github.com/sahirbhatnagar/sail}}{\footnotesize \par}
		\item \textbf{\footnotesize{}\texttt{ggmix}}{\footnotesize{} \textendash{} Bhatnagar, Oualkacha, Yang, Greenwood (2018+, preprint)}\\
		{\footnotesize{}\url{https://github.com/sahirbhatnagar/ggmix}}{\footnotesize \par}
		\item \textbf{\footnotesize{}\texttt{casebase}}{\footnotesize{} \textendash{} Bhatnagar$^1$, Turgeon$^1$, Yang, Hanley and Saarela (2018+, preprint)}\\
		{\footnotesize{}\url{https://cran.r-project.org/package=casebase}}{\footnotesize \par}
\end{itemize}
\end{comment}
	
%\footnotetext[1]{\scriptsize{joint co-authors}}

\ctable[caption={Overview of our software packages for the three methods developped in the thesis. \textbf{Model} describes the type of loss function that has been developped in the thesis for a given method. \textbf{Penalty} are the penalty functions that can applied to the loss function. \textbf{Feature} describes the defining characteristics of the method. \textbf{Data} describes the inputs required for these methods, where $x$ is the design matrix, $y$ is the response, $e$ is a single exposure variable and $\boldsymbol{\Psi}$ is an empirical covariance matrix. See Chapter~\ref{ch:litreview} for more details.},label=tab:over,pos=h!,doinside=\footnotesize]{lccc}{
	}{
	\FL
	& \textbf{\texttt{eclust}}   & \textbf{\texttt{sail}} & \textbf{\texttt{ggmix}} \ML
	\multicolumn{1}{m{2cm}}{\textbf{Model}}     \\
	\rowcolor{whitesmoke}
	\hspace*{0.4cm}Least-Squares & \cmark & \cmark & \cmark   \\
	\hspace*{0.4cm}Binary Classification & \cmark &     &      \\
	\multicolumn{1}{m{2cm}}{\textbf{Penalty}}     \\
	\rowcolor{whitesmoke}
	\hspace*{0.4cm}Ridge        & \cmark &          & \cmark \\
	\hspace*{0.4cm}Lasso        & \cmark & \cmark   & \cmark    \\
	\rowcolor{whitesmoke}
	\hspace*{0.4cm}Elastic Net & \cmark &           & \cmark    \\
	\hspace*{0.4cm}Group Lasso &  &  \cmark  & \cmark     \ML
	\multicolumn{1}{m{2cm}}{\textbf{Feature}} & \\
	\rowcolor{whitesmoke}
	\hspace*{0.4cm}Interactions & \cmark & \cmark       &     \\
	\hspace*{0.4cm}Flexible Modeling & \cmark &  \cmark         &     \\
	\rowcolor{whitesmoke}
	\hspace*{0.4cm}Random Effects &          &          & \cmark     \ML
	\multicolumn{1}{m{2cm}}{\textbf{Data}} & $(x,y,e)$ & $(x,y,e)$ & $(x,y,\boldsymbol{\Psi})$  \LL
}