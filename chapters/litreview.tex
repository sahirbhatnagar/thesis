%-----------------------------------------------------------------------------
\chapter{Literature Review\label{ch:litreview}}
%-----------------------------------------------------------------------------




\begin{comment}
It is easy to write more than you truly need, so try to keep it as limited as possible.  
(1) The problems of high dimension regressions such as overfitting and redundancy of variables.  
(2) then  penalization generally as a solution to (1).  
You might I suppose mention very briefly the alternative solutions like apriori dimension reduction or forward selection, but I would spend as little time as possible on other methods (acknowledging their existence merely and saying your thesis focuses on penalization).  
(3) then L1 methods including lasso \& group lasso (do you discuss elastic net? if so then you might need to include this too). Here I think you should be quite detailed in terms of the theory, how the penalization parameters are chosen, convergence, etc. 
(4) Then something about other structured L1 penalizations that have been proposed.  There are quite a few examples of penalties built for specific applications, and maybe you could find a few such examples and cite them. Not comprehensively.  Then you can point to the sail chapter as a new structured penalty.  
(5) A brief intro to linear mixed models followed by why naive penalization violates the normality of residuals to motivate your ggmix chapter.    I'd stop there
\end{comment}

The Literature Review is comprised of five sections. The first is a description of three general analytic strategies for high-dimensional data. The second and third sections describe two penalization methods that this thesis builds upon, namely the lasso and the group lasso. 
For each method we detail the algorithms used to fit these models and their convergence properties. In the fourth section we introduce penalized interaction models. This is followed by an introduction to penalized linear mixed models.



\section{High-dimensional regression methods}

In this thesis, we consider the prediction of an outcome variable $y$ observed on $n$ individuals from $p$ variables, where $p$ is much larger than $n$. 
Challenges in this high-dimensional context include not only building a good predictor which will perform well in an independent dataset, but also being able to interpret the factors that contribute to the predictions. 
This latter issue can be very challenging in ultra-high dimensional predictor sets. 
For example, multiple different sets of covariates may provide equivalent measures of goodness of fit~\citep{fan2014challenges}, and therefore how does one decide which are important? 
With the advent of high-throughput technologies in genomics and brain imaging studies, computational approaches to variable selection have become increasingly important. 
Broadly speaking, there are three main approaches to analyze high-dimensional data: 1) univariate regression followed by a multiple testing correction 2) multivariable penalized regression and 3) dimension reduction followed by a multivariable regression. We briefly introduce each of these analytic strategies below.


\subsection{Univariate regression} \label{sec:single}

Genome-wide association studies (GWAS) have become the standard method for analyzing genetic datasets. A GWAS consists of a series of univariate regressions followed by a multiple testing correction. 
This approach is simple and easy to implement, and has successfully identified thousands of genetic variants associated with complex diseases (\url{https://www.genome.gov/gwastudies/}). 
Despite these impressive findings, the discovered markers have only been able to explain a small proportion of the phenotypic variance known as the missing heritability problem~\citep{manolio2009finding}.
One plausible explanation is that there are many causal variants that each explain a small amount of variation with small effect sizes~\citep{yang2010common}.
GWAS are likely to miss these true associations due to the stringent significance thresholds required to reduce the number of false positives~\citep{manolio2009finding}.
Most statistical methods for performing multiple testing adjustments assume weak dependence among the variables being tested~\citep{leek2008general}. 
Dependence among multiple tests can lead to incorrect Type 1 error rates~\citep{lin2013test} and highly variable significance measures~\citep{leek2008general}. 
Even in the presence of weakly dependent variables, adjusting for multiple tests in whole genome studies can result in low power. 
Furthermore, the univariate regression approach does not allow for modeling the joint effect of many variants which may be biologically more plausible~\citep{schadt2009molecular}. 
In the next section, we introduce multivariable penalized regression approaches which have been proposed to address some of these limitations. 



\subsection{Multivariable penalized regression} \label{sec:pen}
%\subsubsection{Variable selection and variable screening}
For $n$ observations and $p$ covariates, consider the multiple linear regression model \mbox{$\by = \bX \bbeta + \bs{\varepsilon}$}, where $\by$ is a $n$-length response vector, $\bX$ is an $n \times p$ design matrix, $\bbeta$ is a $p$-length coefficient vector and $\bs{\varepsilon}$ is a $n$-length error vector. 
The least squares estimate is given by $ \widehat{\bbeta} = \left( \bX^T \bX  \right)^{-1} \bX^T \by $. In high-dimensional data, the problem is that $\bX^T \bX$ is singular because the number of covariates greatly exceeds the number of subjects. 
For example DNA microarrays measure the expression of approximately 20,000 genes. 
However, due to funding constraints, the sample size is often less than a few hundred. 
A common solution to this problem is through penalized regression, i.e., apply a constraint on the values of $\bbeta$. The problem can be formulated as finding the vector $\bbeta$ that minimizes the penalized sum of squares:
\begin{equation}
\underbrace{\sum\limits_{i=1}^{n} \left( y_i - \beta_0 - \sum\limits_{j=1}^{p}X_{ij}\beta_j \right)^2}_{\textrm{goodness of fit}} +  \underbrace{\sum\limits_{j=1}^{p} p(\beta_j;\lambda, \gamma)}_{\textrm{penalty}} \label{eq:penalisedlikelihood}
\end{equation}
The first term of~\eqref{eq:penalisedlikelihood} is the squared loss of the data and can be generalized to any loss function while the second term is a penalty which depends on non-negative tuning parameters $\lambda$ and $\gamma$ that control the amount of shrinkage to be applied to $\bbeta$ and the degree of concavity of the penalty function, respectively. 
Several penalty terms have been developed in the literature. Ridge regression places a bound on the square of the coefficients ($\ell_2$ penalty)~\citep{hoerl1970ridge} which has the effect of shrinking the magnitude of the coefficients. 
This however does not produce parsimonious models as none of the coefficients can be shrunk to exactly 0. The Lasso~\citep{tibshirani1996regression} overcomes this problem by placing a bound on the sum of the absolute values of the coefficients ($\ell_1$ penalty) which sets some of them to 0, thereby simultaneously performing model selection. The Lasso, along with other forms of penalization (e.g. SCAD~\cite{fan2001variable}, Fused Lasso~\citep{tibshirani2005sparsity}, Adaptive Lasso~\citep{zou2006adaptive}, Relaxed Lasso~\citep{meinshausen2007relaxed}, MCP~\citep{zhang2010nearly}) have proven successful in many practical problems.
Despite these encouraging results, such methods have low sensitivity in the presence of high empirical correlations between covariates because only one variable tends to be selected from the group of correlated or nearly linearly dependent variables~\citep{buhlmann2013correlated}. 
As a consequence, there is rarely consistency on which variable is chosen from one dataset to another (e.g. in cross-validation folds). This behavior may not be well suited to certain genomic datasets in which large sets of predictors are highly correlated (e.g. a regulatory module) and are also associated with the response. 
The elastic net was proposed to benefit from the strengths of ridge regression's treatment of correlated variables and lasso's sparsity~\citep{zou2005regularization}. By placing both an $\ell_1$ and $\ell_2$ penalty on $\bbeta$, the elastic net achieves model parsimony while yielding similar regression coefficients for correlated variables. 
These methods however do not take advantage of the grouping structure of the data. For example, cortical thickness measurements from magnetic resonance imaging (MRI) scans are often grouped into cortical regions of the Automated Anatomical Labelling (AAL) atlas~\citep{tzourio2002automated}. 
Genes involved in the same cellular process (e.g. KEGG pathway~\citep{kanehisa2008kegg}) can also be placed into biologically meaningful groups. When regularizing with the $\ell_1$ penalty, each variable is selected individually regardless of its position in the design matrix. 
Existing structures between the variables (e.g. spatial, networks, pathways) are ignored even though in many real-life applications the estimation can benefit from this prior knowledge in terms of both prediction accuracy and interpretability~\citep{bach2012structured}. 
The group lasso~\citep{yuan2006model} (and generalizations thereof) overcomes this problem by producing a structured sparsity~\citep{bach2012structured}, i.e., given a predetermined grouping of non-overlapping variables, all members of the group are either zero or non-zero. 
The main drawback when applying these methods to genomic data is that these groups may not be known \textit{a priori}. Known pathways may not be relevant to the response of interest and the study of inferring gene networks is still in its infancy. 




%Many of the methods used in the studies mentioned in Sections~\ref{sec:missing},~\ref{sec:epi} and~\ref{sec:net} are limited to marginal regression models, i.e., looking at one locus at a time and then subsequently applying a multiple testing adjustment. 
%Although , the single-marker test approach has several limitations. First, the system level changes which are believed to be initiated by the environment, will induce very strong correlations between the genes (e.g. Figure~\ref{fig:COPD}). 
%However, most statistical methods for performing multiple testing adjustments assume weak dependence among the variables being tested~\citep{leek2008general}. 
%Dependence among multiple tests can lead to incorrect Type 1 error rates~\citep{lin2013test} and highly variable significance measures~\citep{leek2008general}. 
%It is even more difficult to declare significant true-positive interaction terms, i.e., interaction with the environmental factor, because the environmental variable is common to all models being tested. 
%Second, the single marker gene environment interaction model does not allow for modeling the joint effect of many genes which is biologically more plausible given that whole networks are more likely to be associated with disease than just a single gene. 
%Third, even in the presence of weakly dependent variables, adjusting for multiple tests in whole genome studies can result in low power.


%If many variables are highly correlated, interpretation may be improved by acknowledging the existence of an underlying or latent factor generating these patterns. In consequence, many authors have suggested a two-step procedure where the first step is to cluster or group variables in the design matrix in an interpretable way, and then to perform  model fitting in the second step using a summary measure of each group of variables.


%Predicting a phenotype and understanding which variables improve that prediction are two very challenging and overlapping problems in analysis of high-dimensional data such as those
%arising from genomic and brain imaging studies. It is often believed that the number of truly important predictors is small relative to the total number of variables, making computational approaches to variable selection and dimension reduction extremely important.

%There are several advantages to these two-step methods.  Through the reduction of the dimension of the model, the results are often more stable with smaller prediction variance, and through identification of sets of correlated variables, the resulting clusters can provide an easier route to interpretation. From a practical point of view, two-step approaches are both flexible and easy to implement because efficient algorithms exist for both clustering (e.g.~\citep{fastclust}) and model fitting (e.g.~\citep{friedman2010regularization,gglasso,kuhn2008caret}), particularly in the case when the outcome variable is continuous. 


\subsection{Dimension reduction together with regression}
Due to the unknown grouping problem, several authors have suggested a two-step procedure where they first cluster or group variables in the design matrix and then subsequently proceed to model fitting where the feature space is some summary measure of each group. 
This idea dates back to 1957 when Kendall~\citep{kendall1975multivariate} first proposed using principal components in regression. 
Hierarchical clustering based on the correlation of the design matrix has also been used to create groups of genes in microarray studies and for each level of hierarchy, the cluster average was used as the new set of potential predictors in forward-backward selection~\citep{hastie2001supervised} or the lasso~\citep{park2007averaged}.
\cite{buhlmann2013correlated} proposed a bottom-up agglomerative clustering algorithm based on canonical correlations and used the group lasso on the derived clusters. 
There are some advantages to these methods over the ones previously mentioned in Sections~\ref{sec:single} and~\ref{sec:pen}. 
First, the results may be more interpretable than the traditional lasso (and related methods) because the non-zero components of the prediction model represent sets of genes as opposed to individual ones. 
Second, by using genes which cluster well, we bias the inputs towards correlated sets of genes which are more likely to have similar function. 
Third, taking a summary measure of the resulting clusters can reduce the variance in prediction (overfitting) due to the compressed dimension of the feature space. 
Lastly, from a practical point of view this approach is flexible and easy to implement because efficient algorithms exist for both clustering~\citep{fastclust} and model fitting~\citep{friedman2010regularization,gglasso}. 

In this context, we introduce a new two-step procedure called \texttt{eclust}~\citep{bhatnagar2018analytic} in Chapter 5 of the thesis. Our method is motivated by the fact that exposure variables (e.g. smoking) can alter correlation patterns between clusters of high-dimensional variables, i.e., alter network properties of the variables. However, it is not well understood whether such altered clustering is informative in prediction. In this paper, we explore whether use of exposure-dependent clustering relationships in dimension reduction can improve predictive modelling in a two-step framework. 

A limitation of two-step methods is that the clustering is done in an unsupervised manner, i.e., the clusters do not use the response information. 
This has the effect of assigning similar coefficient values to correlated features. 
\cite{witten2014cluster} proposed a method which encourages features that share an association with the response to take on similar coefficient values. 
This is useful in situations where only a fraction of the features in a cluster are associated with the response. 


\begin{comment}
\ctable[caption={Summary of methods used in simulation study},label=tab:methods,pos=h!,doinside=\footnotesize]{LLLLL}{
}{
\FL
General Approach              & Summary Measure of Feature Clusters & Regression Model & Description \ML
Univariate (uni) & NA  & linear model & \multicolumn{1}{m{5cm}}{Fit a joint model on top 5\% most marginally associated predictors}\\
\addlinespace[5pt] 
Penalization (pen)  & NA & ridge, lasso, scad, elasticnet, mcp & \multicolumn{1}{m{5cm}}{no grouping information is used} \\
\addlinespace[5pt] 
Group Penalization (group)  & NA & group lasso & \multicolumn{1}{m{5cm}}{groups are pre-determined via hierarchical clustering on gene expression correlation matrix}\\
\addlinespace[5pt] 
Cluster (clust) & principal component, average, sparse principal component & linear model, lasso, elasticnet & \multicolumn{1}{m{5cm}}{fit model using summary measure of hierarchical clusters as predictors. clusters are automatically chosen using the \texttt{dynamicTreeCut}~\cite{langfelder2008defining} algorithm} \\
\addlinespace[5pt] 
Environment Cluster (Eclust) & principal component, average, sparse principal component & linear model, lasso, elasticnet & \multicolumn{1}{m{5cm}}{use environment to inform the clustering, then fit model on summary measures of the derived clusters. clusters are chosen using \texttt{kmeans} with $K=2$} \\
\LL
}
\end{comment}

%\section{penalization generally as a solution to (1)}
%You might I suppose mention very briefly the alternative solutions like apriori dimension reduction or forward selection, but I would spend as little time as possible on other methods (acknowledging their existence merely and saying your thesis focuses on penalization).



%\section{The lasso and group lasso}
%Here I think you should be quite detailed in terms of the theory, how the penalization parameters are chosen, convergence, etc.



%%%%%%%%%%% Lasso gradient descent

\section{Lasso}\label{sec:lasso}
Consider the multiple linear regression model \mbox{$\by = \beta_0 + \bX \bbeta + \bs{\varepsilon}$}, where $\by \in \mathbb{R}^n$ is the response, $\bX \in \mathbb{R}^{n \times p}$ is the design matrix, $\beta_0 \in \mathbb{R}$ is the intercept, $\bbeta \in \mathbb{R}^p$ is the coefficient vector corresponding to $\bX$ and $\bs{\varepsilon} \in \mathbb{R}^n$ is a vector of iid random errors. 
For least-squares loss, the lasso estimator~\citep{tibshirani1996regression,zou2006adaptive} is defined as
\begin{equation}
\widehat{\bbeta}(\lambda) = \argmin_{(\beta_0,\bbeta)} \frac{1}{2}  \sum_{i=1}^{n}w_i (y_i -\beta_0- (\bX \bbeta)_i)^2 + \lambda \sum_{j=1}^{p} v_j \abs{\beta_j} \label{eq:lasso_estimator}
\end{equation}
where $(\bX \bbeta)_i$ is the $i$th element of the $n$-length vector $\bX\bbeta$, $\;\lambda > 0$ is a tuning parameter which controls the amount of regularization, $w_i$ is a known weight for the $i$th observation, and $v_j$ is the penalty factor for the $j$th covariate. 
These penalty factors are known and allow parameters to be penalized differently. 
In particular, when $v_j = 1$ for $j = 1, \ldots,p$ then all parameters are regularized equally by $\lambda$, and when $v_j=0$ the $j$th covariate is not penalized, i.e., it will always be included in the model. 
Note also that the intercept is not penalized. 
The estimator~\eqref{eq:lasso_estimator} simultaneously does variable selection and shrinks the regression coefficients towards 0. 
Depending on the choice of $\lambda$, $\widehat{\beta}_j(\lambda)=0$ for some $j$'s, and $\widehat{\beta}_j(\lambda)$ can be thought of as a shrunken least squares estimator~\citep{buhlmann2011statistics}. 
It is worth noting that~\eqref{eq:lasso_estimator} is a convex optimization problem and thus can be solved very efficiently using a block coordinate descent algorithm~\citep{tseng2009coordinate,friedman2007pathwise} for which we provide further details below. Other algorithms for solving this problem exist including LARS~\citep{efron2004least} and the homotopy algorithm~\citep{osborne2000new}, but these have been largely preceded by the coordinate descent algorithm due to its speed and computational efficiency.

\subsection{Block coordinate descent algorithm} \label{ap:cgd}

In a series of seminal papers, Tseng lays the groundwork for a general purpose block coordinate descent algorithm (CGD)~\citep{tseng2009coordinate,tseng1988coordinate,tseng2001convergence} to minimize the sum of a smooth function $f$ (i.e. continuously differentiable) and a separable convex function $P$ of the form
\begin{equation}
Q_{\lambda}(\bTheta) = \argmin_{\bTheta}\; f(\bTheta) + \lambda P(\bTheta) \label{eq:tseng}
\end{equation}
At each iteration, the algorithm approximates $f(\bTheta)$ in~\eqref{eq:tseng} by a strictly convex quadratic function and then applies block coordinate decent to generate a decent direction followed by an inexact line search along this direction~\citep{tseng2009coordinate}. 
For continuously differentiable $f(\cdot)$ and convex and block-separable $P(\cdot)$ \mbox{(e.g. $P(\bbeta) = \sum_i P_i (\beta_i)$)},~\cite{tseng2009coordinate} show that the solution generated by the CGD method is a stationary point of~\eqref{eq:tseng} if the coordinates are updated in a Gauss-Seidel manner, i.e., $Q_{\lambda}(\bTheta)$ is minimized with respect to one parameter while holding all others fixed. 
The separability of the penalty function into a sum of functions of each individual parameter is the key to applying this algorithm to lasso type problems. 
%The CGD algorithm can thus be run in parallel and therefore suited for large $p$ settings. 
Indeed, the CGD algorithm has been successfully applied in fixed effects models~\citep{meier2008group,friedman2010regularization} and linear mixed models~\citep{schelldorfer2011estimation}. Following~\cite{tseng2009coordinate}, the general purpose CGD algorithm for solving~\eqref{eq:tseng} is given by Algorithm~\ref{alg:cgd}. 

\begin{algorithm}[htbp]
	\SetAlgoLined
	%	\KwResult{Write here the result }
	Set the iteration counter $k \leftarrow 0$ and choose initial values for the parameter vector $\bTheta^{(0)}$\;
	\Repeat{convergence criterion is satisfied}{
		Approximate the Hessian $\nabla^2 f(\bTheta^{(k)})$ by a symmetric matix $H^{(k)}$:
		\begin{equation}
		H^{(k)} = \diag \left[ \min \left\lbrace \max \left\lbrace \left[ \nabla^2 f(\bTheta^{(k)})\right] _{jj}, 10^{-2} \right\rbrace 10^{9} \right\rbrace\right]_{j = 1, \ldots, p} \label{eq:Hk}
		\end{equation}
		\For{$ j =1, \ldots, p$}{
			
			Solve the descent direction $d^{(k)} \coloneqq d_{H^{(k)}}(\Theta_{j}^{(k)})$ \;

				\begin{equation}
				d_{H^{(k)}}(\Theta_{j}^{(k)}) \leftarrow \argmin_{d} \left\lbrace \nabla f(\Theta_{j}^{(k)}) d + \frac{1}{2} d^2 H^{(k)}_{j j} + \lambda P(\Theta_{j}^{(k)} + d) \right\rbrace \label{eq:descentdirection}
				\end{equation}
			
	Choose a stepsize\;
	\begin{equation*}
	\alpha_j^{(k)} \leftarrow \tm{line search given by the Armijo rule}
	\end{equation*}
	Update\;
	\begin{equation*}
	\widehat{\Theta}_j^{(k+1)} \leftarrow \widehat{\Theta}_j^{(k)} + \alpha_j^{(k)}d^{(k)}
	\end{equation*}
}
$k \leftarrow k +1$
}
\caption{Coordinate Gradient Descent Algorithm to solve~\eqref{eq:tseng}} \label{alg:cgd}
\end{algorithm}

\FloatBarrier

The Armijo rule is defined as follows~\citep{tseng2009coordinate}:
\begin{tcolorbox}
	Choose $\alpha_{init}^{(k)}>0$ and let $\alpha^{(k)}$ be the largest element of $\left\lbrace \alpha_{init}^k \delta^r \right\rbrace_{r = 0,1,2,\ldots} $ satisfying
	\begin{equation}
	Q_{\lambda}(\Theta_j^{(k)} + \alpha^{(k)} d^{(k)}) \leq Q_{\lambda} (\Theta_j^{(k)}) + \alpha^{(k)}\varrho \Delta^{(k)}
	\end{equation}
	where $0 < \delta <1$, $0 < \varrho <1$, $0 \leq \gamma < 1$ and
	\begin{equation}
	\Delta^{(k)} \coloneqq \nabla f(\Theta_j^{(k)})d^{(k)} + \gamma (d^{(k)})^2 H^{(k)}_{jj} + \lambda P(\Theta_j^{(k)} + d^{(k)}) - \lambda P(\Theta_j^{(k)})
	\end{equation}
\end{tcolorbox}
Common choices for the constants are $\delta=0.1$, $\varrho=0.001$, $\gamma = 0$, $\alpha_{init}^{(k)} = 1$ for all $k$~\citep{bertsekas1999nonlinear}.
In what follows, we use Algorithm~\ref{alg:cgd} to solve the lasso estimator with least-squares loss given by~\eqref{eq:lasso_estimator}. Without loss of generality, we assume the penalty factors ($v_j$) are all equal to 1. 


\subsubsection{Descent Direction}
For simplicity, we remove the iteration counter $(k)$ from the derivation below.\\ For \mbox{$\Theta_j^{(k)} \in \left\lbrace \beta_1, \ldots, \beta_p \right\rbrace$}, let
\begin{equation}
d_{H}(\Theta_{j}) = \argmin_{d} G(d)  \label{eq:argminGd}
\end{equation}
where
\[ G(d) =  \nabla f(\Theta_{j}) d + \frac{1}{2} d^2 H_{j j} + \lambda |\Theta_{j} + d| \]
Since $G(d)$ is not differentiable at $-\Theta_j$, we calculate the subdifferential $\partial G(d)$ and search for $d$ with $0 \in \partial G(d)$:
\begin{equation}
\partial G(d) = \nabla f(\Theta_{j}) + d H_{j j} + \lambda u   \label{eq:subdiff}
\end{equation}
where
\begin{equation}
u = \begin{cases}
1 & \tm{if\quad}d > -\Theta_j \\
-1 & \tm{if\quad} d < -\Theta_j\\
[-1,1] & \tm{if\quad} d = \Theta_j
\end{cases}
\end{equation}
We consider each of the three cases in~\eqref{eq:subdiff} below
\begin{enumerate}
	\item $d > -\Theta_j$
	\begin{align}
	\partial G(d) & = \nabla f(\Theta_{j}) + d H_{j j} + \lambda = 0 \nonumber \\
	d & = \frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}}  \nonumber
	\end{align}
	Since $\lambda>0$ and $H_{jj}>0$, we have
	\begin{equation*}
	\frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}} > \frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} = d \overset{\tm{def}}{>} -\Theta_j
	\end{equation*}
	The solution can be written compactly as
	\begin{equation*}
	d = \tm{mid}\left\lbrace \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}, -\Theta_j ,\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \right\rbrace
	\end{equation*}
	where $\tm{mid}\left\lbrace a,b,c \right\rbrace$ denotes the median (mid-point) of $a,b,c$.
	\item $d < -\Theta_j$
	\begin{align}
	\partial G(d) & = \nabla f(\Theta_{j}) + d H_{j j} - \lambda = 0 \nonumber \\
	d & = \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}  \nonumber
	\end{align}
	Since $\lambda>0$ and $H_{jj}>0$, we have
	\begin{equation*}
	\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} < \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}} = d \overset{\tm{def}}{<} -\Theta_j
	\end{equation*}
	Again, the solution can be written compactly as
	\begin{equation*}
	d = \tm{mid}\left\lbrace \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}, -\Theta_j ,\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \right\rbrace
	\end{equation*}
	
	\item $d_j = -\Theta_j$\\
	There exists $u \in [-1,1]$ such that
	\begin{align*}
	\partial G(d) & = \nabla f(\Theta_{j}) + d H_{j j} + \lambda u = 0 \nonumber \\
	d & = \frac{-(\nabla f(\Theta_{j}) + \lambda u)}{H_{j j}}  \nonumber
	\end{align*}
	For $-1 \leq u \leq 1$, $\lambda>0$ and $H_{jj}>0$ we have
	\begin{equation*}
	\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \leq  d \overset{\tm{def}}{=} -\Theta_j \leq \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}
	\end{equation*}
	The solution can again be written compactly as
	\begin{equation*}
	d = \tm{mid}\left\lbrace \frac{-(\nabla f(\Theta_{j}) - \lambda)}{H_{j j}}, -\Theta_j ,\frac{-(\nabla f(\Theta_{j}) + \lambda)}{H_{j j}} \right\rbrace
	\end{equation*}
	
\end{enumerate}
We see all three cases lead to the same solution for~\eqref{eq:argminGd}. Therefore the descent direction for $\Theta_j^{(k)} \in \left\lbrace \beta_1, \ldots, \beta_p \right\rbrace$ for the $\ell_1$ penalty is given by
\begin{equation}
d = \tm{mid}\left\lbrace \frac{-(\nabla f(\beta_{j}) - \lambda)}{H_{j j}}, -\beta_j ,\frac{-(\nabla f(\beta_{j}) + \lambda)}{H_{j j}} \right\rbrace  \label{eq:d}
\end{equation}

\subsubsection{Solution for the $\beta$ parameter} \label{ap:beta}
If the Hessian $\nabla^2f(\bTheta^{(k)}) >0$ then $H^{(k)}$ defined in~\eqref{eq:Hk} is equal to $\nabla^2f(\bTheta^{(k)})$. Using $\alpha_{init} = 1$, the largest element of $\left\lbrace \alpha_{init}^{(k)} \delta^r \right\rbrace_{r = 0, 1, 2, \ldots}$ satisfying the Armijo Rule inequality is reached for $\alpha^{(k)} = \alpha_{init}^{(k)}\delta^0 = 1$. The Armijo rule update for the $\bbeta$ parameter is then given by
\begin{equation}
\beta_j^{(k+1)} \leftarrow \beta_j^{(k)} + d^{(k)}, \qquad j=1, \ldots, p \label{eq:betaupdate}
\end{equation}
Substituting the descent direction given by~\eqref{eq:d} into~\eqref{eq:betaupdate} we get
\begin{equation}
\beta_j^{(k+1)} = \tm{mid}\left\lbrace \beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) - \lambda)}{H_{j j}}, 0,\beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) + \lambda)}{H_{j j}}  \right\rbrace \label{eq:betaMidpoint}
\end{equation}
We can further simplify this expression. First, we can re-write the loss function in~\eqref{eq:lasso_estimator} as
\begin{align}
g(\bbeta^{(k)}) & = \frac{1}{2} \sum_{i=1}^{n} w_i\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} - X_{ij}\beta_j^{(k)} \right) ^2
\end{align}
The gradient and Hessian are given by
\begin{align}
\nabla f(\beta_j^{(k)}) \coloneqq \frac{\partial}{\partial \beta_j^{(k)}}g(\bbeta^{(k)}) & = - \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} - X_{ij}\beta_j^{(k)} \right)  \label{eq:grad}\\
H_{jj} \coloneqq \frac{\partial^2}{\partial {\beta_j^{(k)}}^2}g(\bbeta^{(k)}) & = \sum_{i=1}^{n} w_i X_{ij}^2  \label{eq:hessian}
\end{align}
Substituting~\eqref{eq:grad} and~\eqref{eq:hessian} into $\beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) - \lambda)}{H_{jj}}$ %~\eqref{eq:betaMidpoint} we get
\begin{align}
& \beta_j^{(k)}+ \frac{  \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} - X_{ij}\beta_j^{(k)} \right)  + \lambda }{\sum_{i=1}^{n} w_i X_{ij}^2} \nonumber \\
& = \beta_j^{(k)}+ \frac{ \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} \right) + \lambda}{\sum_{i=1}^{n} w_i X_{ij}^2} - \frac{\sum_{i=1}^{n} w_i X_{ij}^2\beta_j^{(k)}  }{\sum_{i=1}^{n} w_i X_{ij}^2} \nonumber \\
& =  \frac{  \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} \right) + \lambda}{\sum_{i=1}^{n} w_i X_{ij}^2} \label{eq:midleft}
\end{align}
Similarly, substituting~\eqref{eq:grad} and~\eqref{eq:hessian} in $\beta_j^{(k)}+ \frac{-(\nabla f(\beta_j^{(k)}) + \lambda)}{H_{jj}}$ we get
\begin{align}
\frac{  \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} \right) - \lambda}{\sum_{i=1}^{n} w_i X_{ij}^2} \label{eq:midright}
\end{align}
Finally, substituting~\eqref{eq:midleft} and~\eqref{eq:midright} into~\eqref{eq:betaMidpoint} we get
\begin{align}
\beta_j^{(k+1)} & = \tm{mid}\left\lbrace \frac{  \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} \right) - \lambda}{\sum_{i=1}^{n} w_i X_{ij}^2}, 0,\frac{  \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} \right) + \lambda}{\sum_{i=1}^{n} w_i X_{ij}^2} \right\rbrace \nonumber \\
& = \frac{\mathcal{S}_{\lambda}\left( \sum_{i=1}^{n} w_i X_{ij}\left(  y_i - \sum_{\ell \neq j}X_{i\ell} \beta_\ell^{(k)} \right)\right) }{\sum_{i=1}^{n} w_i X_{ij}^2} \label{eq:betaUpdateSoft}
\end{align}

Where $\mathcal{S}_{\lambda}(x)$ is the soft-thresholding operator
\begin{equation*}
\mathcal{S}_{\lambda}(x) = \tm{sign}(x)(|x| - \lambda)_+
\end{equation*}
$\textrm{sign}(x)$ is the signum function
\begin{equation*}
\textrm{sign}(x) = \begin{cases}
-1 & x<0\\
0 & x= 0\\
1 & x>0
\end{cases}
\end{equation*}
and $(x)_+ = \max(x, 0)$. Since there is a closed form solution, which can be computed very quickly for each parameter update (given by~\eqref{eq:betaUpdateSoft}), the CGD algorithm is an attractive approach for solving the lasso estimator.  


\subsection{Lambda sequence} \label{subsec:lasso_lambda_seq}
In general, the solution to~\eqref{eq:lasso_estimator} is computed over a decreasing sequence of values for the tuning parameter $\lambda$, beginning with the smallest value $\lambda_{max}$ for which the entire coefficient vector $\widehat{\bbeta}=\mb{0}_p$ ~\citep{friedman2010regularization}. To determine $\lambda_{max}$, we turn to the Karush-Kuhn-Tucker (KKT) optimality conditions for~\eqref{eq:lasso_estimator}. These conditions can be written as
\begin{equation}
\begin{aligned}
\frac{1}{v_j} \sum_{i=1}^{n}w_i X_{ij}\left(  y_i - \sum_{j=1}^{p}X_{ij}\hat\beta_j \right) =  \lambda \gamma_j, \\
\gamma_j \in \begin{cases}
\tm{sign}(\hat{\beta}_j) & \tm{if} \quad \hat{\beta}_j \neq 0 \\
[-1,1] & \tm{if}\quad \hat{\beta}_j = 0
\end{cases}, \qquad \tm{for }j=1, \ldots, p  
\end{aligned}\label{eq:kktsolved}
\end{equation}
where $\gamma_j$ is the subgradient of the function $f(x) = \abs{x}$ evaluated at $x = \hat{\beta}_j$. From~\eqref{eq:kktsolved}, we can solve for the smallest value of $\lambda$ such that the entire vector ($\hat{\beta}_1, \ldots, \hat{\beta}_p$) is 0. This is given by
\begin{equation}
\lambda_{max} = \max_j \left\lbrace \abs{ \frac{1}{v_j} \sum_{i=1}^{n}w_i X_{ij}y_i }\right\rbrace , \quad j=1, \ldots, p
\end{equation}
Following~\cite{friedman2010regularization}, we can choose $\tau\lambda_{max}$ to be the smallest value of tuning parameters $\lambda_{min}$, and construct a
sequence of $K$ values decreasing from $\lambda_{max}$ to $\lambda_{min}$ on the log scale. The defaults are set to $K = 100$, $\tau = 0.01$ if $n < p $ and $\tau = 0.001$ if $n \geq p $. The optimal value of $\lambda$ can be chosen using 5-fold or 10-fold cross-validation. For least-squares loss, this corresponds to choosing the $\lambda$ which minimizes the mean squared error. 


\subsection{Warm starts} \label{subsec:lasso_warmstarts}
The way in which we have derived the sequence of tuning parameters using the KKT conditions, allows us to exploit warm starts which has been shown to lead to computational speedups~\citep{friedman2010regularization}. 
That is, the solution $\widehat{\bTheta}$ for $\lambda_k$ is used as the initial value $\bTheta^{(0)}$ for $\lambda_{k+1}$. 

\subsection{Adaptive lasso} \label{subsec:adaptive_lasso}
It has been shown that the lasso estimator can be produce biased estimates for large coefficients and give inconsistent variable selection results at the optimal $\lambda$ for prediction, i.e., many noise features are included in the prediction model~\citep{zou2006adaptive}.
To overcome the bias problems of the lasso,~\cite{zou2006adaptive} proposed the adaptive lasso which allows a different amount of shrinkage for each regression coefficient using adaptive weights. 
Adaptive weighting has been shown to construct oracle procedures~\citep{fan2001variable}, i.e., asymptotically, it performs as well as if the true model were given in advance. 
The adaptive lasso can be described as a two-stage procedure: 
\begin{enumerate}
	\item Calculate the initial regression estimates $\widehat{\bbeta}_{init}$ from~\eqref{eq:lasso_estimator}  
	\item Refit~\eqref{eq:lasso_estimator} using penalty factors $v_j$ equal to $1/\abs{\hat{\beta}_{init,j}}$ for $j=1,\ldots,p$. 
\end{enumerate}
As we can see from the weights, the adaptive lasso will shrink larger coefficients less which leads to consistent variable selection results under weaker conditions than the lasso~\citep{buhlmann2011statistics}. 
We detail the adaptive lasso procedure in Algorithm~\ref{alg:adaptivelasso}. 

\begin{algorithm}
	\begin{enumerate}
		\item For a decreasing sequence $\lambda = \lambda_{max}, \ldots,\lambda_{min}$, fit the lasso with $v_j=1$ for $j=1,\ldots, p$ 
		\item Use cross-validation or a data splitting procedure to determine the optimal value for the tuning parameter: $\lambda^{[opt]} \in \left\lbrace \lambda_{max},\ldots, \lambda_{min} \right\rbrace$
		\item Let $\hat{\beta}_{init,j}^{[opt]}$ for $j=1, \ldots,p$ be the coefficient estimates corresponding to the model at $\lambda^{[opt]}$
		\item Set the weights to be $v_j = \left(\abs{\hat{\beta}_{init,j}^{[opt]}}\right)^{-1}$ for $j=1, \ldots, p$
		\item Refit the lasso with the weights defined in step 4), and use cross-validation or a data splitting procedure to choose the optimal value of $\lambda$
	\end{enumerate}
	\caption{Adaptive lasso algorithm \label{alg:adaptivelasso}}
\end{algorithm}

\section{Group Lasso}

One main drawback of the lasso is that it ignores the grouping structure of the design matrix. When given a predetermined grouping of non-overlapping variables, we would want all members of the group to be either zero or non-zero. For example, when dealing with categorical predictors where each factor is expressed through a set of indicator variables, removing an irrelevant factor is equivalent to setting the coefficients of the indicator variables to 0. In an additive model, where each variable is projected on to a set of basis function, e.g. $f_j(X_j) = \sum_{\ell = 1}^{m_j} \psi_{j\ell}(X_j) \beta_{j\ell}$, we would want all $\left\lbrace \beta_{j\ell}\right\rbrace_{\ell=1}^{m_j}$ to be either zero or non-zero. This key difference between the lasso and group lasso penalty is shown in Figure~\ref{fig:lasso_grouplasso_comparison}. 
Suppose we want to predict an individual's credit card balance from their age and height using the following additive model:
\begin{equation}
\tm{credit card balance} = \beta_0 + \beta_{11}\tm{age} + \beta_{12}\tm{age}^2 + \beta_{21}\tm{height} + \beta_{22}\tm{height}^2 + \varepsilon \label{eq:credit_card}
\end{equation}  
In Figures~\ref{fig:1} and~\ref{fig:2} we see that both the lasso and group lasso set the linear and quadratic terms for height ($\hat\beta_{21},\hat\beta_{22}$) to 0. However, the lasso estimates only a nonzero quadratic term for age ($\hat\beta_{11}=0, \hat\beta_{12}\neq 0$) while the group lasso estimates both linear and quadratic terms to be nonzero ($\hat\beta_{11}\neq 0, \hat\beta_{12}\neq 0$). We now provide details on the group lasso estimator.

\input{figs/lasso_grouplasso_comparison}

Assume that the predictors in the design matrix $\bX \in \mathbb{R}^{n \times p}$ belong to $K$ groups and define the cardinality of index set $I_{k}$ to be $p_{k}$. These groups are known \textit{a priori} such that $(1,2,\ldots,p)=\bigcup_{k=1}^{K}I_{k}$, and are also non-overlapping, i.e., $I_{k}\bigcap I_{k^{\prime}}=\emptyset$ for $k\neq k^{\prime}$.
Therefore, group $k$ contains $p_{k}$ predictors corresponding to $\bX^{(k)}$, i.e., the columns of the design matrix $X_{j}$ for $j\in I_{k}$, and $1\le k\le K.$ The intercept belongs to its own group, i.e., $I_{1}=\{1\}$. The group lasso partitions the variable coefficients into $K$ groups $\boldsymbol{\beta}=([\boldsymbol{\beta}^{(1)}]^{\intercal},[\boldsymbol{\beta}^{(2)}]^{\intercal},\cdots,[\boldsymbol{\beta}^{(K)}]^{\intercal})^{\intercal}$, where $\bbeta^{(k)}$ denotes the segment of $\bbeta$ corresponding to group $k$. 
For least-squares loss, the group lasso estimator~\citep{yuan2006model} is given by:
\begin{equation}
\widehat{\bbeta}(\lambda) = \argmin_{(\beta_0,\bbeta)} \frac{1}{2}  \sum_{i=1}^{n}w_i (y_i -\beta_0- (\bX \bbeta)_i)^2 + \lambda\sum_{k=1}^{K}v_k\Vert\boldsymbol{\beta}^{(k)}\Vert_{2} \label{eq:glasso_estimator}
\end{equation}
where $\norm{\bbeta^{(k)}}_2 = \sqrt{\sum_{j \in I_k} \beta^{2}_j }\;$ and $\lambda > 0$ is the tuning parameter. As in the lasso estimator~\eqref{eq:lasso_estimator}, there are both observation weights $w_i$, and penalty factors $v_k \geq 0$ which control the relative strength of the terms within the group lasso penalty. These penalty factors are often set to $\sqrt{p_k}$~\citep{yuan2006model}. Note that the same penalty factor is applied to all the coefficients in a group. Solving the group lasso estimator is more challenging than the lasso since there is no closed form solution for~\eqref{eq:glasso_estimator}. In the next section, we detail a majorization-minimization (MM) type algorithm~\citep{yang2015fast,MM1} used to solve~\eqref{eq:glasso_estimator}. 


\subsection{Groupwise majorization descent algorithm}

This description of the groupwise majorization descent (GMD) algorithm used to solve~\eqref{eq:glasso_estimator} follows mainly from~\cite{yang2015fast}. The main difference here is that we consider a more general loss function of the form

\begin{equation}
L(\bbeta\mid\bD)=\frac{1}{2}\left[\by-\widehat{\by}\right]^{\top}\mathbf{W}\left[\by-\widehat{\by}\right] \label{eq:general_form}
\end{equation}

where $\widehat{\by}=\hat{\beta_0}+\bX\widehat\bbeta$, $\bD$ is the working data $\lbrace \by, \bX \rbrace$, and $\bW$ is an $n \times n$ nonsingular and known weight matrix. This weight matrix can be used when the elements of $\by$ are correlated as is done in generalized least squares. The original proposal in~\cite{yang2015fast} is a special case of~\eqref{eq:general_form} where $\bW$ is a diagonal matrix with entries equal to $w_i$. The loss function~\eqref{eq:general_form} satisfies the quadratic majorization (QM) condition, since $L(\bbeta\mid\bD)$ is differentiable as a function of $\bbeta$, i.e., $\nabla L(\bbeta|\bD)=-\left(\by-\hat{\by}\right)^{\top}\mathbf{W}\bX$, and there exists
a $p\times p$ matrix $\bH=\bX^{\trans}\mathbf{W}\bX$ which only depends on the data $\bD$, such that for all $\bbeta,\bbeta^{*}$,
\begin{equation}
L(\bbeta\mid\bD)\le L(\bbeta^{*}\mid\bD)+(\bbeta-\bbeta^{*})^{\trans}\nabla L(\bbeta^{*}|\bD)+\frac{1}{2}(\bbeta-\bbeta^{*})^{\trans}\bH(\bbeta-\bbeta^{*}).\label{QM1}
\end{equation}
We can exploit the fact that the loss function~\eqref{eq:general_form} satisfies the QM condition to majorize the loss function in~\eqref{eq:glasso_estimator} by~\eqref{QM1}, and that the penalty term $\sum_{k=1}^{K}v_k\Vert\boldsymbol{\beta}^{(k)}\Vert_{2}$ in~\eqref{eq:glasso_estimator} is separable with respect to the indices of the variables $k=1, \ldots, K$ to performs groupwise updates for each $\bbeta^{(k)}$. 

Let $\widetilde{\bbeta}$ denote the current solution of $\bbeta$ and define $\bH^{(k)}$ as the sub-matrix of $\bH$ corresponding to group $k$. For example, if group 2 is $\{2,4\}$ then $\bH^{(2)}$ is a $2\times2$ matrix with
\[
\bH^{(2)}=\left[\begin{array}{cc}
h_{2,2} & h_{2,4}\\
h_{4,2} & h_{4,4}
\end{array}\right],
\]
where $h_{i,j}$ is the $i,j$th entry of the $\bH$ matrix. Write $\bbeta$ such that $\bbeta^{(k^{\prime})}=\widetilde{\bbeta}^{(k^{\prime})}$ for $k^{\prime}\ne k$. Given $\bbeta^{(k^{\prime})}=\widetilde{\bbeta}^{(k^{\prime})}$ for $k^{\prime}\ne k$, the estimator for $\bbeta^{(k)}$ is given by
\begin{equation}
\widehat{\bbeta}^{(k)}(\lambda) = \arg\min_{\boldsymbol{\beta}^{(k)}}L(\bbeta\mid\bD)+\lambda v_{k}\Vert \bbeta^{(k)}\Vert_{2}.\label{GMDeq1}
\end{equation}
Using $\bbeta-\widetilde{\bbeta}=(\underbrace{0,\ldots,0}_{k-1},\bbk-\bbkt,\underbrace{0,\ldots,0}_{K-k})$ and the majorization defined in~\eqref{QM1} we can write
\begin{equation}
L(\bbeta\mid\bD)\le L(\widetilde{\bbeta}\mid\bD)-(\bbk-\bbkt)^{\trans}U^{(k)}+\frac{1}{2}(\bbk-\bbkt)^{\trans}\bH^{(k)}(\bbk-\bbkt).\label{GMDeq2}
\end{equation}
where
\begin{align}
U^{(k)} & =\frac{\partial}{\partial\bbk}L(\bbeta\mid\bD)=-\left(\by-\hat{\by}\right)^{\top}\mathbf{W}\mathbf{X}^{(k)},\label{eq:gradientj-1}\\
\mathbf{H}^{(k)} & =\frac{\partial^{2}}{\partial\bbk\partial \bbeta^{(k)\top}}L(\bbeta\mid\bD)=(\mathbf{X}^{(k)})^{\top}\mathbf{W}\mathbf{X}^{(k)}.\label{eq:hessianj-1}
\end{align}

The upper bound in~\eqref{GMDeq2} can be majorized even further to avoid expensive matrix multiplications and storing $\bH^{(k)}$ in memory. Let $\eta_{k}$ be the largest eigenvalue of $\bH^{(k)}$. We set $\gamma_{k}=(1+\varepsilon^{*})\eta_{k}$, where $\varepsilon^{*}=10^{-6}$ and substitute $\gamma_{k}$ for $\bH^{(k)}$ in~\eqref{GMDeq2}:
\begin{equation}
L(\bbeta\mid\bD)\leq L(\widetilde{\bbeta}\mid\bD)-(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}U_{(k)}+\frac{1}{2}\gamma_{k}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)}).\label{GMDeq3-1}
\end{equation}
It is important to note that the inequality strictly holds unless when $\bbeta^{(k)}=\widetilde{\bbeta}^{(k)}$. Substituting the upper bound in~\eqref{GMDeq3-1} for $L(\bbeta\mid\bD)$ in~\eqref{GMDeq1} we get the following estimator for $\bbeta^{(k)}$
\begin{equation}
\arg\min_{\bbeta^{(k)}}L(\widetilde{\bbeta}\mid\bD)-(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}U^{(k)}+\frac{1}{2}\gamma_{k}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})^{\trans}(\bbeta^{(k)}-\widetilde{\bbeta}^{(k)})+\lambda v_{k}\Vert\bbeta^{(k)}\Vert_{2}.\label{GMDeq4-1}
\end{equation}

Let $\widetilde{\bbeta}^{(k)}(\textrm{new})$ be the solution to~\eqref{GMDeq4-1} which has a simple closed-from expression:
\begin{equation}
\widetilde{\bbeta}^{(k)}(\textrm{new})=\frac{1}{\gamma_{k}}\left(U^{(k)}+\gamma_{k}\widetilde{\bbeta}^{(k)}\right)\left(1-\frac{\lambda v_{k}}{\Vert U^{(k)}+\gamma_{k}\widetilde{\bbeta}^{(k)}\Vert_{2}}\right)_{+}.\label{GMDeq5-1}
\end{equation}

Algorithm~\ref{alg1} summarizes the details of GMD for the group lasso with least-squares loss function given by~\eqref{eq:general_form}. The GMD algorithm has the strict descent property and converges to the right answer. The proof follows directly from Section 3.1 of~\cite{yang2015fast}. 

In Figure~\ref{fig:gglasso_maj} we provide an illustration of the quadratic majorization technique for updating a parameter $\beta_j$. The solution lies at the minimum of the $F$ curve but we cannot solve for this directly since there is no closed form solution. Instead we majorize $F$ using $Q_1$ which is a function consisting of the quadratic approximation of $F$ plus the penalty term evaluated at $\beta_j^{(m)}$. The minimum of $Q_1$, for which there is a closed form solution, corresponds to the next iteration $\beta_j^{(m+1)}$. We then majorize again using $Q_2$ and solve for the minimum. This process is repeated until convergence.  

 
\fig{maj-crop}{gglasso_maj}{16cm}{Illustration of the quadratic majorization technique}{Illustration of the quadratic majorization technique}


\begin{algorithm}
	\begin{enumerate}
		\item For $k=1,\ldots,K$, compute $\gamma_k$, the largest eigenvalue of $\bH^{(k)} = (\mathbf{X}^{(k)})^{\top}\mathbf{W}\mathbf{X}^{(k)}$
		\item Initialize $\widetilde \bbeta$.
		\item Repeat the following cyclic groupwise updates until convergence:
		\begin{enumerate}
			\item[---] for $k=1,\ldots,K$, do step (3.1)--(3.3)
			\begin{enumerate}
				\item[3.1]
				Compute $U(\widetilde \bbeta )=-\nabla L(\widetilde \bbeta | \bD)= -\left(\by-\hat{\by}\right)^{\top}\mathbf{W}\mathbf{X}^{(k)}$
				\item[3.2]
				Compute
				$
				\widetilde \bbeta^{(k)}(\textrm{new}) = \frac{1}{\gamma_k}\left( U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \right)\left(1-\frac{\lambda v_k}{\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2}\right)_{+} .
				$
				\item[3.3]
				Set $\widetilde \bbeta^{(k)}=\widetilde \bbeta^{(k)}(\textrm{new})$.
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
	\caption{The GMD algorithm for group lasso with least-squares loss function given by~\eqref{eq:general_form}. \label{alg1}}
\end{algorithm}


\subsection{Lambda sequence}
Similar to Section~\ref{subsec:lasso_lambda_seq}, we compute the solution to~\eqref{eq:glasso_estimator} over a decreasing sequence of values for the tuning parameter $\lambda$ starting with $\lambda_{max}$. From the update formula~\eqref{GMDeq5-1} we have that for all $k$
$$
\begin{cases}\label{KKTcond1}
\widetilde \bbeta^{(k)} = \frac{1}{\gamma_k}\left( U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \right)\left(1-\frac{\lambda v_k}{\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2}\right) & \qquad\textrm{if}\;\;\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2 > \lambda v_{k}\\\label{KKTcond2}
\widetilde \bbeta^{(k)} = \boldsymbol{0} & \qquad\textrm{if}\;\;\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2 \leq \lambda v_{k}\;.
\end{cases}
$$
We can then directly obtain the KKT conditions for $k=1, \ldots, K$:
\begin{equation}
\begin{aligned}
-U^{(k)}+\lambda v_{k}\cdot\frac {\widetilde\bbeta^{(k)} }{\Vert\widetilde\bbeta^{(k)}\Vert_2}=\boldsymbol{0}\qquad\textrm{if }\widetilde\bbeta^{(k)}\neq \boldsymbol{0},\\
\left\Vert
U^{(k)}
\right\Vert_2 \le\lambda w_{k}\qquad\textrm{if }\widetilde\bbeta^{(k)}=\boldsymbol{0}\; .
\end{aligned} \label{eq:kkt_glasso}
\end{equation}

Using~\eqref{eq:kkt_glasso} we can solve for the smallest value of $\lambda$ such that the entire vector $\left\lbrace \widehat{\bbeta}^{(k)}\right\rbrace_{k=1}^{K}$ is 0. This is given by
 \begin{align}
 \lambda_{max} & = \max_k  \frac{1}{v_k} \norm{  U^{(k)}}_2 , \quad k=1, \ldots, K, \; v_k \neq 0
 \end{align}

\subsection{Warm starts and adaptive group lasso}

Warm starts can also be implemented for the group lasso as described in Section~\ref{subsec:lasso_warmstarts} for the lasso. Furthermore, the adaptive group lasso can be computed using Algorithm~\ref{alg:adaptivelasso}; the main difference is that the coefficient estimates in Step 1 are obtained from a group lasso fit and the weights for group $k$ are given by $v_k = \left(\norm{\widehat{\bbeta}^{(k)}}_2\right)^{-1}$. 





\begin{comment}
\subsection{Convergence}

We can prove the strict descent property of GMD by using the MM principle \citep{MM1,hunter2004tutorial,MM08}. Define
\begin{equation}
Q(\bbeta \mid \bD)=L(\widetilde \bbeta \mid \bD)-(\bbeta^{(k)}-\widetilde \bbeta^{(k)})^{\trans}
U^{(k)}+\frac{1}{2} \gamma_k (\bbeta^{(k)}-\widetilde \bbeta^{(k)})^{\trans} ( \bbeta^{(k)}- \widetilde \bbeta^{(k)})+\lambda w_k \Vert \bbeta^{(k)}\Vert_2.
\end{equation}
Obviously, $Q(\bbeta \mid \bD)=L(\bbeta \mid \bD)+\lambda w_k \Vert \bbeta^{(k)}\Vert_2$ when $\bbeta^{(k)}=\widetilde \bbeta^{(k)}$ and
(\ref{GMDeq3}) shows that
$Q(\bbeta \mid \bD) > L(\bbeta \mid \bD)+\lambda w_k \Vert \bbeta^{(k)}\Vert_2$ when $\bbeta^{(k)} \neq \widetilde \bbeta^{(k)}$.
After updating $\widetilde \bbeta^{(k)}$ using (\ref{GMDeq5}), we have
\begin{eqnarray*}
	L(\widetilde \bbeta^{(k)}(\textrm{new}) \mid \bD)+\lambda w_k \Vert \widetilde \bbeta^{(k)}(\textrm{new}) \Vert_2
	&  \le &  Q(\widetilde \bbeta^{(k)}(\textrm{new})  \mid \bD)\\
	& \le & Q(\widetilde \bbeta  \mid \bD) \\
	& = & L(\widetilde \bbeta \mid \bD)+\lambda w_k \Vert \widetilde \bbeta^{(k)}\Vert_2.
\end{eqnarray*}
Moreover, if $\widetilde \bbeta^{(k)}(\textrm{new}) \neq \widetilde \bbeta^{(k)}$, then the first inequality becomes
\begin{eqnarray*}
	L(\widetilde \bbeta^{(k)}(\textrm{new}) \mid \bD)+\lambda w_k \Vert \widetilde \bbeta^{(k)}(\textrm{new}) \Vert_2
	&  < &  Q(\widetilde \bbeta^{(k)}(\textrm{new})  \mid \bD).
\end{eqnarray*}
Therefore, the objective function is strictly decreased after updating all groups in a cycle, unless the solution does not change after each groupwise update. If this is the case,
we can show that the solution must satisfy the KKT conditions, which means that the algorithm converges and finds the right answer. To see this,
if $\widetilde \bbeta^{(k)}(\textrm{new}) = \widetilde \bbeta^{(k)}$ for all $k$, then by the update formula \eqref{GMDeq5-1} we have that for all $k$
\begin{align}\label{KKTcond1}
\widetilde \bbeta^{(k)} = \frac{1}{\gamma_k}\left( U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \right)\left(1-\frac{\lambda w_k}{\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2}\right) \qquad\textrm{if }\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2 > \lambda w_{k},\\\label{KKTcond2}
\widetilde \bbeta^{(k)} = \boldsymbol{0} \qquad\textrm{if }\Vert U^{(k)}+\gamma_k \widetilde \bbeta^{(k)} \Vert_2 \leq \lambda w_{k}.
\end{align}
By straightforward algebra  we obtain the KKT conditions:
\begin{align*}
-U^{(k)}+\lambda w_{k}\cdot\frac {\widetilde\bbeta^{(k)} }{\Vert\widetilde\bbeta^{(k)}\Vert_2}=\boldsymbol{0}\qquad\textrm{if }\widetilde\bbeta^{(k)}\neq \boldsymbol{0},\\
\left\Vert
U^{(k)}
\right\Vert_2 \le\lambda w_{k}\qquad\textrm{if }\widetilde\bbeta^{(k)}=\boldsymbol{0},
\end{align*}
where $k=1,2,\ldots,K$. Therefore, if the objective function stays unchanged after a cycle, the algorithm necessarily converges to the right
answer.
\end{comment}



\section{Penalized interaction models}


%In the context of studying gene environment interactions, all previously mentioned methods can yield results where the main effects are 0 but the cross terms are not. While there may exist situations where this can occur, such behavior is generally not biologically plausible in genomics~\citep{bakermans2015hidden}. 
%Two other arguments against such behavior have to do with statistical power and practical importance; 1) large main effects are more likely to lead to detectable interactions than small ones~\citep{cox1984interaction} and 2) a data collector cares about the number of variables they need to \textit{measure} to make predictions at a future time~\citep{bien2013lasso}. 
%Moreover, the proposed clustering methods are based on features from all observations. Any correlation patterns specific to a subgroup of patients (e.g. Figures~\ref{fig:COPDnevercorr} and~\ref{fig:COPDcurrentcorr}) may become diluted when looking at the overall correlation matrix. 
%Therefore the focus of my doctoral research will be on developing a method that can identify previously unknown sets of highly correlated genes that interact with the environment to explain phenotypic variation, with a focus on prediction accuracy and model interpretability. 
In this section, we introduce penalized regression methods in the context of interaction models. We consider a regression model for an $n$-length outcome variable $\by$ which follows an exponential family. Let $E=(e_1, \ldots, e_n)$ be the binary environment or exposure vector and $\bX$ the $n \times p $ matrix of high-dimensional data. Consider the regression model with main effects and their interactions with $E$:
\begin{align}
g(\bmu)  = & \beta_0  + \underbrace{\beta_1 X_{1} + \cdots + \beta_p X_p + \beta_{E} E}_{\tm{main effects}} + \underbrace{\alpha_{1E} (X_1 E) + \cdots + \alpha_{pE} (X_p E)}_{\tm{interactions}} \label{eq:linpred}
\end{align}
where $g(\cdot)$ is a known link function and $\bmu = \e\left[\by|\bX, E, \bbeta,\balpha\right]$.
Due to the large number of parameters to estimate with respect to the number of observations, we might consider adding the lasso penalty introduced in Section~\ref{sec:lasso}. A natural extension of the lasso penalty applied to the interaction model~\eqref{eq:linpred} is
\begin{equation}
\argmin_{\beta_0, \bbeta, \boldsymbol{\alpha} }  \frac{1}{2} \norm{Y - g(\bmu)}_2^2 + \lambda \left( \norm{\bbeta}_1 + \norm{\balpha}_1    \right)\; , \label{eq:lassolikelihood}
\end{equation}
where $\norm{Y - g(\bmu)}_2^2 = \sum_i (y_i - g(\mu_i))^2$, $\norm{\bbeta}_1 = \sum_j | \beta_j  |$, $\norm{\balpha}_1 = \sum_j | \alpha_j  |$ and $\lambda \geq 0$ is the tuning parameter that can set some of the coefficients to zero when sufficiently large. A limitation of~\eqref{eq:lassolikelihood} is that the selected model may have main effects that are 0 but the corresponding interaction terms are not. This is due to the penalty which treats both main effects and interactions equally. While there may exist situations where this can occur, statisticians have long argued that large main effects are more likely to lead to detectable interactions than small ones~\citep{cox1984interaction} and therefore, main effects should enter the model before interactions. This is known as the strong heredity principle~\citep{chipman1996bayesian}:
\begin{equation}
\hat{\alpha}_{jE} \neq 0 \qquad \Rightarrow \qquad \hat{\beta}_j \neq 0 \qquad \tm{and} \qquad \hat{\beta}_E \neq 0   \label{eq:heredity}
\end{equation}
In words, the interaction term will only have a non-zero estimate if its corresponding main effects are estimated to be non-zero. One benefit brought by hierarchy is that the number of measured variables can be reduced, referred to as practical sparsity~\citep{she2014group,bien2013lasso}. For example, a model involving $X_1, E, X_1 \cdot E$ is more parsimonious than a model involving $X_1, E, X_2 \cdot E$, because in the first model a researcher would only have to measure two variables compared to three in the second model.

There have been several proposals for modeling interactions with the strong heredity constraint in the penalization literature including Composite Absolute Penalties (CAP)~\citep{zhao2009composite}, Variable selection using Adaptive Nonlinear Interaction Structures in High dimensions (VANISH)~\citep{radchenko2010variable}, Strong Hierarchical Lasso (hierNet)~\citep{bien2013lasso}, Group-Lasso Interaction Network (glinternet)~\citep{lim2015learning}, Group Regularized Estimation under Structural Hierarchy (GRESH)~\citep{she2014group} and a Framework for Modeling Interactions with a Convex Penalty (FAMILY)~\citep{haris2014convex}. A popular approach to achieve this structured sparsity is via the group lasso penalty where each group contains both main effects and their corresponding interactions. For example, consider a two-way interaction model of the form
\begin{align*}
y = &\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \beta_3 X_3 \\
&+ \beta_{12} X_1 X_2 + \beta_{13} X_1X_3 + \beta_{23} X_2X_3 + \varepsilon
\end{align*}
A grouping structure that would satisfy the strong heredity property is:
\begin{table}[H]
	\centering
	\begin{tabular}{|c|c|c|c|ccc|ccc|ccc|}
		\hline
		group     & 1         & 2         & 3         & \multicolumn{3}{c|}{4}            & \multicolumn{3}{c|}{5}            & \multicolumn{3}{c|}{6}            \\
		parameter & $\beta_1$ & $\beta_2$ & $\beta_3$ & $\beta_1$ & $\beta_2$ & $\beta_{12}$ & $\beta_1$ & $\beta_3$ & $\beta_{13}$ & $\beta_2$ & $\beta_3$ & $\beta_{23}$ \\ \hline
	\end{tabular}
\end{table}
A limitation of this approach is that its impossible to select one variable without selecting all the groups containing it~\citep{jacob2009group}. This implies that if a main effect has been selected, all the interaction terms with that main effect will necessarily be selected as well which can be an issue when the truth contains no interactions. 

In Chapter 3 of the thesis, we introduce a new structured sparsity called \texttt{sail} which satisfies the strong heredity property while overcoming this issue, i.e., a selected main effect does not automatically imply that the corresponding interactions will also be selected. Furthermore, our method can accommodate non-linear interaction effects on the response. The context of Chapter 3 is summarized in Table~\ref{tab:sailcomparison}.

\ctable[caption={Overview of methods with structured sparsity for penalized regression models},label=tab:sailcomparison,pos=H,doinside=\footnotesize]{llc}{
}{
\FL
Type      & Method   & Software \ML
\multicolumn{1}{m{1cm}}{Linear}    & \multicolumn{1}{m{6cm}}{\texttt{CAP}~\citep{zhao2009composite} }        &   \xmark    \\
& \multicolumn{1}{m{6cm}}{\texttt{SHIM}~\citep{choi2010variable}}        &   \xmark    \\
& \multicolumn{1}{m{6cm}}{\texttt{hiernet}~\citep{bien2013lasso}}        &   \texttt{hierNet(x, y)}    \\
& \multicolumn{1}{m{6cm}}{\texttt{GRESH}~\citep{she2014group} }        &  \xmark     \\
& \multicolumn{1}{m{6cm}}{\texttt{FAMILY}~\citep{haris2016convex}}    &  \texttt{FAMILY(x, z, y)}   \\
& \multicolumn{1}{m{6cm}}{\texttt{glinternet}~\citep{lim2015learning} }    & \texttt{glinternet(x, y)}  \\			   	 	
& \multicolumn{1}{m{6cm}}{\texttt{RAMP}~\citep{hao2018model}}        & \texttt{RAMP(x, y)}  \\ 
& \multicolumn{1}{m{6cm}}{\texttt{LassoBacktracking}~\citep{shah2016modelling}   }        & \texttt{LassoBT(x, y)}  \ML
\multicolumn{1}{m{2cm}}{Non-linear} 	& \multicolumn{1}{m{8cm}}{\texttt{VANISH}~\citep{radchenko2010variable} }        & \xmark  \\
& \multicolumn{1}{m{6cm}}{\texttt{sail} (\textcolor{red}{Chapter 3 of the thesis})}        & \texttt{\textcolor{blue}{sail(x, y, e)}}  \LL
}




\begin{comment}
However this method and related ones are restricted to all pairwise interactions between $p$ measured variables. Therefore I will explore methods that impose a strong hierarchy in the context of gene environment interactions. 
In order to address these issues, we propose to extend the model of Choi \textit{et al.}~\citep{choi2010variable} to simultaneously perform variable selection, estimation and impose the strong heredity principle in the context of high dimensional interactions with the environment (HD$\times E$). To do so, we follow Choi and reparametrize the coefficients for the interaction terms as $\alpha_{jE} = \gamma_{jE} \beta_j \beta_E$. Plugging this into~\eqref{eq:linpred1}:
\begin{align}
g(\bmu)  = & \beta_0  + \beta_1 \widetilde{X}_{1} + \cdots + \beta_q \widetilde{X}_q + \beta_{E} E + \gamma_{1E}\beta_1 \beta_E (\widetilde{X}_1 E) + \cdots + \gamma_{qE}\beta_q \beta_E (\widetilde{X}_q E)    \label{eq:linpred2}
\end{align}
where $\widetilde{\bx} = (\widetilde{X}_1, \ldots, \widetilde{X}_q)$ are the cluster representatives derived in phase 2 and $q <p$. This reparametrization directly enforces the strong heredity principle (Eq.~\eqref{eq:heredity}), i.e., if either main effect estimates are 0, then $\hat{\alpha}_{jE}$ will be zero and a non-zero interaction coefficient implies non-zero $\hat{\beta}_j$ and $\hat{\beta}_E$. To perform variable selection in this new parametrization, we follow Choi \textit{et al.}~\cite{choi2010variable} and penalize $\bs{\gamma} = \left(\gamma_{1E}, \ldots, \gamma_{pE}\right)$ instead of penalizing $\balpha$ as in~\eqref{eq:lassolikelihood}, leading to the following penalized least squares criterion:
\begin{equation}
\argmin_{\beta_0, \bbeta, \bs{\gamma} }  \frac{1}{2} \norm{Y - g(\bmu)}^2 + \lambda_\beta \left(w_1 \beta_1 + \cdots + w_q \beta_q + w_E \beta_E   \right) + \lambda_\gamma  \left( w_{1E} \gamma_{1E} + \cdots + w_{qE}\gamma_{qE}         \right) \label{eq:lassolikelihood2}
\end{equation} 
where $g(\bmu)$ is from~\eqref{eq:linpred2}, $\lambda_\beta$ and $\lambda_\gamma$ are tuning parameters and $\mb{w} = \left(w_{1}, \ldots, w_q, w_{1E}, \ldots, w_{qE}\right)$ are prespecified adaptive weights. The $\lambda_\beta$ tuning parameter controls the amount of shrinkage applied to the main effects, while $\lambda_\gamma$ controls the interaction estimates and allows for the possibility of excluding the interaction term from the model even if the corresponding main effects are non-zero. 
%The adaptive weights serve as a way of allowing parameters to be penalized differently. Furthermore, adaptive weighting~\citep{zou2006adaptive} has been shown to construct oracle procedures~\citep{fan2001variable}, i.e., asymptotically, it performs as well as if the true model were given in advance. The oracle property is achieved when the weights are a function of any root-$n$ consistent estimator of the true parameters e.g. maximum likelihood (MLE) or ridge regression estimates. 
It can be shown that the procedure in~\eqref{eq:lassolikelihood2} asymptotically possesses the oracle property~\citep{choi2010variable}, even when the number of parameters tends to $\infty$ as the sample size increases, if the weights are chosen such that
\begin{equation}
w_j = \left | \frac{1}{\hat{\beta}_j} \right|, \quad w_{jE} = \left | \frac{\hat{\beta}_j \hat{\beta}_E}{\hat{\alpha}_{jE}} \right| \quad \tm{ for }j=1, \ldots, q   \label{eq:weights}
\end{equation}
where $\hat{\beta}_j$ and $\hat{\alpha}_{j}$ are the MLEs, \textit{using the transformed variables}, from~\eqref{eq:linpred1} or the ridge regression estimates when $q > n$. The rationale behind the data-dependent $\hat{\bs{w}}$ is that as the sample size grows, the weights for the truly zero predictors go to $\infty$ (which translates to a large penalty), whereas the weights for the truly non-zero predictors converge to a finite constant~\citep{zou2006adaptive}. 

 While each method has their own merit, including that they are all convex optimization problems, they all contain complex penalty functions which are hard to interpret and lead to computationally expensive fitting algorithms. On the other hand, the objective function in~\eqref{eq:lassolikelihood2} can be solved using an iterative approach (by first fixing $\bbeta$ and then $\balpha$) which simplifies to a LASSO type problem; one that has been extensively studied, is well understood and can be solved efficiently using existing software (e.g. \texttt{glmnet}~\citep{friedman2010regularization}). A limitation of this approach is that the optimization problem is non-convex, arising from the reparametrization of $\balpha$ as a product of optimization variables $(\bbeta, \bs{\gamma})$, and hence convergence to the global minimum is not guaranteed~\citep{choi2010variable}. We argue that since there is only one $E$, and that $\widetilde{X}$ is much smaller in dimension, finding a solution is much more likely. 

%We argue here that in our applications, we are more concerned with identifying the truly associated parameters (as given by an oracle procedure) than finding a global minimum which would be guaranteed in a convex formulation. 

To our knowledge, strong hierarchies have never previously been used in HD interaction analysis in genomics or brain imaging studies. Furthermore, the specific choices of weights proposed here, i.e., based on the transformed variables from phase 2, have not been previously used. Choi \textit{et al.}~\citep{choi2010variable} estimated their weights simultaneously, but this would not be feasible in HD data. Finally, the adaptation to interactions with one key $E$ variable is specific to our situation and this leads to computational efficiencies. These three points constitute novel aspects of this thesis. I have a working implementation of this, and am in the process of conducting simulation studies.
\end{comment}


\section{Penalized linear mixed models}
(5) A brief intro to linear mixed models followed by why naive penalization violates the normality of residuals to motivate your ggmix chapter.


%\section{you will need a section on things like gradient descent and other algorithms for finding solutions efficiently}